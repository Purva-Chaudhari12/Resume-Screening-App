{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0039921a",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4001d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c14c0",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5bc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\resume_project\\UpdatedResumeDataSet (3).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cff7568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume\n",
       "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
       "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
       "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
       "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
       "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95a6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd2eec",
   "metadata": {},
   "source": [
    "# Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ededc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Java Developer               84\n",
       "Testing                      70\n",
       "DevOps Engineer              55\n",
       "Python Developer             48\n",
       "Web Designing                45\n",
       "HR                           44\n",
       "Hadoop                       42\n",
       "Blockchain                   40\n",
       "ETL Developer                40\n",
       "Operations Manager           40\n",
       "Data Science                 40\n",
       "Sales                        40\n",
       "Mechanical Engineer          40\n",
       "Arts                         36\n",
       "Database                     33\n",
       "Electrical Engineering       30\n",
       "Health and fitness           30\n",
       "PMO                          30\n",
       "Business Analyst             28\n",
       "DotNet Developer             28\n",
       "Automation Testing           26\n",
       "Network Security Engineer    25\n",
       "SAP Developer                24\n",
       "Civil Engineer               24\n",
       "Advocate                     20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c185808b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Data Science'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mcountplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[1;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass values for both `x` and `y`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2943\u001b[0m plotter \u001b[38;5;241m=\u001b[39m _CountPlotter(\n\u001b[0;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[0;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[0;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[0;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[0;32m   2948\u001b[0m )\n\u001b[0;32m   2950\u001b[0m plotter\u001b[38;5;241m.\u001b[39mvalue_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[0;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[0;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[0;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestablish_variables(x, y, hue, data, orient,\n\u001b[0;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[0;32m   1532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[0;32m   1533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:516\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m    513\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Convert to a list of arrays, the common representation\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(d, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m plot_data]\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# The group names will just be numeric indices\u001b[39;00m\n\u001b[0;32m    519\u001b[0m group_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(plot_data)))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:516\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    513\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Convert to a list of arrays, the common representation\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(d, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m plot_data]\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# The group names will just be numeric indices\u001b[39;00m\n\u001b[0;32m    519\u001b[0m group_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(plot_data)))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:917\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m--> 917\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    919\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Data Science'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.countplot(df['Category'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d340e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['Category'].value_counts()\n",
    "labels = df['Category'].unique()\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.pie(counts,labels=labels,autopct='%1.1f%%',shadow=True, colors=plt.cm.plasma(np.linspace(0,1,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ef849",
   "metadata": {},
   "source": [
    "# Exploring Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a97c061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bc53d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b912e",
   "metadata": {},
   "source": [
    "# Balance Classes (Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0f5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Category Distribution:\n",
      "Category\n",
      "Java Developer               84\n",
      "Testing                      70\n",
      "DevOps Engineer              55\n",
      "Python Developer             48\n",
      "Web Designing                45\n",
      "HR                           44\n",
      "Hadoop                       42\n",
      "Blockchain                   40\n",
      "ETL Developer                40\n",
      "Operations Manager           40\n",
      "Data Science                 40\n",
      "Sales                        40\n",
      "Mechanical Engineer          40\n",
      "Arts                         36\n",
      "Database                     33\n",
      "Electrical Engineering       30\n",
      "Health and fitness           30\n",
      "PMO                          30\n",
      "Business Analyst             28\n",
      "DotNet Developer             28\n",
      "Automation Testing           26\n",
      "Network Security Engineer    25\n",
      "SAP Developer                24\n",
      "Civil Engineer               24\n",
      "Advocate                     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced Category Distribution (After Oversampling):\n",
      "Category\n",
      "Advocate                     84\n",
      "Blockchain                   84\n",
      "ETL Developer                84\n",
      "HR                           84\n",
      "Electrical Engineering       84\n",
      "DevOps Engineer              84\n",
      "Health and fitness           84\n",
      "DotNet Developer             84\n",
      "Network Security Engineer    84\n",
      "Arts                         84\n",
      "Data Science                 84\n",
      "Database                     84\n",
      "Python Developer             84\n",
      "Operations Manager           84\n",
      "Web Designing                84\n",
      "PMO                          84\n",
      "Civil Engineer               84\n",
      "Automation Testing           84\n",
      "Mechanical Engineer          84\n",
      "Hadoop                       84\n",
      "Testing                      84\n",
      "Java Developer               84\n",
      "Business Analyst             84\n",
      "SAP Developer                84\n",
      "Sales                        84\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the original category distribution\n",
    "print(\"Original Category Distribution:\")\n",
    "print(df['Category'].value_counts())\n",
    "\n",
    "# Get the largest category size (i.e., the category with the maximum number of entries)\n",
    "max_size = df['Category'].value_counts().max()\n",
    "\n",
    "# Perform oversampling\n",
    "balanced_df = df.groupby('Category').apply(lambda x: x.sample(max_size, replace=True)).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the dataset to avoid any order bias\n",
    "df = balanced_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the balanced category distribution\n",
    "print(\"\\nBalanced Category Distribution (After Oversampling):\")\n",
    "print(df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b4549",
   "metadata": {},
   "source": [
    "# Cleaning Data:                                      \n",
    "1 URLs,                                                \n",
    "2 hashtags,                                             \n",
    "3 mentions,                                                     \n",
    "4 special letters,                                             \n",
    "5 punctuations:                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16656511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b2c86ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my webiste like is this and a ess it '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanResume(\"my #### $ #  #noorsaeed webiste like is this http://heloword and access it @gmain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e752795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resume'] = df['Resume'].apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba537207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hard working Quick learnerEducation Details June 2014 to May 2017 LLB LAW Mumbai Maharashtra mumbai university January 2014 B Com Commerce Mumbai Maharashtra Mumbai university January 2011 HSC Maharashtra board January 2009 SSC Maharashtra board Advocate Skill Details Company Details company The vidishtra description '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034b60c",
   "metadata": {},
   "source": [
    "# words into categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72994ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af36ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(df['Category'])\n",
    "df['Category'] = le.transform(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c4bd0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 18, 21,  4, 15, 23, 13, 16,  2,  5, 19, 24, 20,  3,  7,  6,  1,\n",
       "       17,  9, 14,  8, 11, 12, 10, 22])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c915950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Data Science', 'HR', 'Advocate', 'Arts', 'Web Designing',\n",
    "#        'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
    "#        'Civil Engineer', 'Java Developer', 'Business Analyst',\n",
    "#        'SAP Developer', 'Automation Testing', 'Electrical Engineering',\n",
    "#        'Operations Manager', 'Python Developer', 'DevOps Engineer',\n",
    "#        'Network Security Engineer', 'PMO', 'Database', 'Hadoop',\n",
    "#        'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],\n",
    "#       dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65059b",
   "metadata": {},
   "source": [
    "# Vactorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e3603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf.fit(df['Resume'])\n",
    "requredTaxt  = tfidf.transform(df['Resume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3c383",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f67afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab60004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, df['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d542f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 7335)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc6048f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 7335)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b1be4",
   "metadata": {},
   "source": [
    "# Now let’s train the model and print the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d24938b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier Results:\n",
      "Accuracy: 0.9952\n",
      "Confusion Matrix:\n",
      "[[11  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        13\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       1.00      1.00      1.00        17\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       1.00      1.00      1.00        17\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       1.00      1.00      1.00        17\n",
      "           9       1.00      1.00      1.00        15\n",
      "          10       1.00      1.00      1.00        19\n",
      "          11       1.00      1.00      1.00        16\n",
      "          12       0.91      1.00      0.95        20\n",
      "          13       1.00      1.00      1.00        17\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        14\n",
      "          16       1.00      1.00      1.00        20\n",
      "          17       1.00      1.00      1.00        17\n",
      "          18       1.00      1.00      1.00        16\n",
      "          19       1.00      1.00      1.00        16\n",
      "          20       1.00      1.00      1.00        20\n",
      "          21       1.00      1.00      1.00        12\n",
      "          22       1.00      1.00      1.00        19\n",
      "          23       1.00      1.00      1.00        17\n",
      "          24       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           1.00       420\n",
      "   macro avg       1.00      0.99      0.99       420\n",
      "weighted avg       1.00      1.00      1.00       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Ensure that X_train and X_test are dense if they are sparse\n",
    "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "\n",
    "# 1. Train KNeighborsClassifier\n",
    "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print(\"\\nKNeighborsClassifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_knn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7913cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC Results:\n",
      "Accuracy: 1.0000\n",
      "Confusion Matrix:\n",
      "[[13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       1.00      1.00      1.00        17\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       1.00      1.00      1.00        17\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       1.00      1.00      1.00        17\n",
      "           9       1.00      1.00      1.00        15\n",
      "          10       1.00      1.00      1.00        19\n",
      "          11       1.00      1.00      1.00        16\n",
      "          12       1.00      1.00      1.00        20\n",
      "          13       1.00      1.00      1.00        17\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        14\n",
      "          16       1.00      1.00      1.00        20\n",
      "          17       1.00      1.00      1.00        17\n",
      "          18       1.00      1.00      1.00        16\n",
      "          19       1.00      1.00      1.00        16\n",
      "          20       1.00      1.00      1.00        20\n",
      "          21       1.00      1.00      1.00        12\n",
      "          22       1.00      1.00      1.00        19\n",
      "          23       1.00      1.00      1.00        17\n",
      "          24       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           1.00       420\n",
      "   macro avg       1.00      1.00      1.00       420\n",
      "weighted avg       1.00      1.00      1.00       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Train SVC\n",
    "svc_model = OneVsRestClassifier(SVC())\n",
    "svc_model.fit(X_train, y_train)\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "print(\"\\nSVC Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_svc)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b8ae7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier Results:\n",
      "Accuracy: 1.0000\n",
      "Confusion Matrix:\n",
      "[[13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       1.00      1.00      1.00        17\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       1.00      1.00      1.00        17\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       1.00      1.00      1.00        17\n",
      "           9       1.00      1.00      1.00        15\n",
      "          10       1.00      1.00      1.00        19\n",
      "          11       1.00      1.00      1.00        16\n",
      "          12       1.00      1.00      1.00        20\n",
      "          13       1.00      1.00      1.00        17\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        14\n",
      "          16       1.00      1.00      1.00        20\n",
      "          17       1.00      1.00      1.00        17\n",
      "          18       1.00      1.00      1.00        16\n",
      "          19       1.00      1.00      1.00        16\n",
      "          20       1.00      1.00      1.00        20\n",
      "          21       1.00      1.00      1.00        12\n",
      "          22       1.00      1.00      1.00        19\n",
      "          23       1.00      1.00      1.00        17\n",
      "          24       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           1.00       420\n",
      "   macro avg       1.00      1.00      1.00       420\n",
      "weighted avg       1.00      1.00      1.00       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Train RandomForestClassifier\n",
    "rf_model = OneVsRestClassifier(RandomForestClassifier())\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"\\nRandomForestClassifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935cb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7594d627",
   "metadata": {},
   "source": [
    "# Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "716d3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('tfidf.pkl','wb'))\n",
    "pickle.dump(svc_model, open('clf.pkl', 'wb'))\n",
    "pickle.dump(le, open(\"encoder.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d636d",
   "metadata": {},
   "source": [
    "# Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3b4dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict the category of a resume\n",
    "def pred(input_resume):\n",
    "    # Preprocess the input text (e.g., cleaning, etc.)\n",
    "    cleaned_text = cleanResume(input_resume) \n",
    "\n",
    "    # Vectorize the cleaned text using the same TF-IDF vectorizer used during training\n",
    "    vectorized_text = tfidf.transform([cleaned_text])\n",
    "    \n",
    "    # Convert sparse matrix to dense\n",
    "    vectorized_text = vectorized_text.toarray()\n",
    "\n",
    "    # Prediction\n",
    "    predicted_category = svc_model.predict(vectorized_text)\n",
    "\n",
    "    # get name of predicted category\n",
    "    predicted_category_name = le.inverse_transform(predicted_category)\n",
    "\n",
    "    return predicted_category_name[0]  # Return the category name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06b1805c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"I am a data scientist specializing in machine\n",
    "learning, deep learning, and computer vision. With\n",
    "a strong background in mathematics, statistics,\n",
    "and programming, I am passionate about\n",
    "uncovering hidden patterns and insights in data.\n",
    "I have extensive experience in developing\n",
    "predictive models, implementing deep learning\n",
    "algorithms, and designing computer vision\n",
    "systems. My technical skills include proficiency in\n",
    "Python, Sklearn, TensorFlow, and PyTorch.\n",
    "What sets me apart is my ability to effectively\n",
    "communicate complex concepts to diverse\n",
    "audiences. I excel in translating technical insights\n",
    "into actionable recommendations that drive\n",
    "informed decision-making.\n",
    "If you're looking for a dedicated and versatile data\n",
    "scientist to collaborate on impactful projects, I am\n",
    "eager to contribute my expertise. Let's harness the\n",
    "power of data together to unlock new possibilities\n",
    "and shape a better future.\n",
    "Contact & Sources\n",
    "Email: 611noorsaeed@gmail.com\n",
    "Phone: 03442826192\n",
    "Github: https://github.com/611noorsaeed\n",
    "Linkdin: https://www.linkedin.com/in/noor-saeed654a23263/\n",
    "Blogs: https://medium.com/@611noorsaeed\n",
    "Youtube: Artificial Intelligence\n",
    "ABOUT ME\n",
    "WORK EXPERIENCE\n",
    "SKILLES\n",
    "NOOR SAEED\n",
    "LANGUAGES\n",
    "English\n",
    "Urdu\n",
    "Hindi\n",
    "I am a versatile data scientist with expertise in a wide\n",
    "range of projects, including machine learning,\n",
    "recommendation systems, deep learning, and computer\n",
    "vision. Throughout my career, I have successfully\n",
    "developed and deployed various machine learning models\n",
    "to solve complex problems and drive data-driven\n",
    "decision-making\n",
    "Machine Learnine\n",
    "Deep Learning\n",
    "Computer Vision\n",
    "Recommendation Systems\n",
    "Data Visualization\n",
    "Programming Languages (Python, SQL)\n",
    "Data Preprocessing and Feature Engineering\n",
    "Model Evaluation and Deployment\n",
    "Statistical Analysis\n",
    "Communication and Collaboration\n",
    "\"\"\"\n",
    "\n",
    "pred(myresume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94f2f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Health and fitness'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"\n",
    "Jane Smith is a certified personal trainer with over 5 years of experience in helping individuals achieve their fitness goals. Specializing in weight loss, strength training, and sports conditioning, Jane has developed personalized workout routines for clients of all ages and fitness levels. She has extensive knowledge in nutrition and exercise science, and uses this to create holistic health and fitness programs that are tailored to individual needs.\n",
    "\n",
    "Jane holds a degree in Exercise Science and is a certified trainer through the National Academy of Sports Medicine (NASM). She has worked with athletes, seniors, and individuals with chronic health conditions, helping them improve their physical well-being and overall quality of life.\n",
    "\n",
    "Her expertise includes:\n",
    "- Weight Loss and Body Composition\n",
    "- Strength Training and Resistance Exercises\n",
    "- Cardio Conditioning\n",
    "- Nutrition Coaching and Meal Planning\n",
    "- Injury Prevention and Rehabilitation\n",
    "- Functional Movement and Flexibility Training\n",
    "- Group Fitness Classes\n",
    "\n",
    "Certifications:\n",
    "- Certified Personal Trainer, NASM\n",
    "- CPR and First Aid Certified\n",
    "- Yoga Instructor (200-Hour Certification)\n",
    "\n",
    "Education:\n",
    "BSc in Exercise Science, ABC University, 2014-2018\n",
    "\n",
    "Work Experience:\n",
    "- Personal Trainer at XYZ Fitness Gym (2018-Present)\n",
    "- Fitness Coach at Wellness Center (2016-2018)\n",
    "\n",
    "Languages:\n",
    "- English (Fluent)\n",
    "- Spanish (Conversational)\n",
    "\"\"\"\n",
    "\n",
    "# Now, test the model with the Health and Fitness-focused resume\n",
    "pred(myresume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6e0c8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Network Security Engineer'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"\n",
    "John Doe is an experienced Network Security Engineer with over 7 years of expertise in designing, implementing, and managing network security infrastructures. Specializing in safeguarding critical network systems, John has worked with various organizations to protect against cyber threats, data breaches, and unauthorized access. He is proficient in deploying firewalls, intrusion detection systems (IDS), VPNs, and network monitoring tools to ensure the integrity and security of networks.\n",
    "\n",
    "John holds a degree in Computer Science and certifications in several cybersecurity domains, including Certified Information Systems Security Professional (CISSP), Certified Ethical Hacker (CEH), and Cisco Certified Network Associate (CCNA). He has extensive experience in troubleshooting and resolving network vulnerabilities, and has played a key role in conducting security audits and risk assessments.\n",
    "\n",
    "Key Skills:\n",
    "- Network Security Architecture\n",
    "- Firewall Management and Configuration\n",
    "- Intrusion Detection and Prevention Systems (IDS/IPS)\n",
    "- Virtual Private Networks (VPNs)\n",
    "- Security Audits and Risk Assessments\n",
    "- Cybersecurity Incident Response\n",
    "- Network Monitoring and Traffic Analysis\n",
    "- Vulnerability Assessment and Penetration Testing\n",
    "- Data Encryption and Secure Communications\n",
    "\n",
    "Certifications:\n",
    "- CISSP (Certified Information Systems Security Professional)\n",
    "- CEH (Certified Ethical Hacker)\n",
    "- CCNA (Cisco Certified Network Associate)\n",
    "- CompTIA Security+\n",
    "\n",
    "Education:\n",
    "BSc in Computer Science, XYZ University, 2012-2016\n",
    "\n",
    "Professional Experience:\n",
    "- Network Security Engineer at ABC Corp (2016-Present)\n",
    "- IT Security Specialist at DEF Solutions (2014-2016)\n",
    "\n",
    "Languages:\n",
    "- English (Fluent)\n",
    "- French (Intermediate)\n",
    "\"\"\"\n",
    "\n",
    "# Now, test the model with the Network Security Engineer-focused resume\n",
    "pred(myresume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f291ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advocate'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"\n",
    "Sarah Williams is a dedicated and skilled advocate with over 10 years of experience in providing legal representation to clients across various sectors, including criminal law, civil litigation, and family law. With a deep understanding of legal procedures and case law, Sarah has successfully handled numerous cases in the courtroom, negotiating favorable settlements and providing expert legal advice to individuals and businesses.\n",
    "\n",
    "She holds a law degree from XYZ University and is a licensed attorney, practicing law in multiple jurisdictions. Sarah is passionate about ensuring justice is served and strives to make legal processes more accessible to her clients. She is known for her excellent research and analytical skills, attention to detail, and commitment to upholding the law with integrity.\n",
    "\n",
    "Key Skills:\n",
    "- Criminal Law\n",
    "- Civil Litigation\n",
    "- Family Law\n",
    "- Contract Law\n",
    "- Legal Research and Writing\n",
    "- Courtroom Advocacy\n",
    "- Legal Counseling and Advice\n",
    "- Client Relationship Management\n",
    "- Legal Compliance and Regulations\n",
    "- Negotiation and Settlement\n",
    "\n",
    "Certifications and Licenses:\n",
    "- Licensed Attorney at Law, XYZ State Bar\n",
    "- Certification in Criminal Law, XYZ University\n",
    "\n",
    "Education:\n",
    "JD in Law, XYZ University, 2010-2013\n",
    "\n",
    "Professional Experience:\n",
    "- Senior Advocate at ABC Law Firm (2016-Present)\n",
    "- Associate Advocate at DEF Legal Group (2013-2016)\n",
    "\n",
    "Languages:\n",
    "- English (Fluent)\n",
    "- Spanish (Conversational)\n",
    "\"\"\"\n",
    "\n",
    "# Now, test the model with the Advocate-focused resume\n",
    "pred(myresume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb1062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
